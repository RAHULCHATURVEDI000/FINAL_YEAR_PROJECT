{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f03c36b9-86f7-48db-871f-ae829c831963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/dhruv/AnacondaInstallation/envs/tf-env/lib/python3.9/site-packages/cv2/qt/plugins\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # Try changing 0 to 1, etc.\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "    exit()\n",
    "cv2.imshow('Frame', frame)\n",
    "cv2.waitKey(0)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5424c702-7361-4462-aa8d-05d09be8b597",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d858c2-dc28-4b9b-9279-1c589e8dac6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 340\n",
      "Testing samples: 86\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(yawn_dir, no_yawn_dir, img_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Load images from directories and prepare them for training.\n",
    "    \n",
    "    Args:\n",
    "        yawn_dir (str): Path to directory containing yawn images\n",
    "        no_yawn_dir (str): Path to directory containing no_yawn images\n",
    "        img_size (tuple): Target size for resizing images\n",
    "        \n",
    "    Returns:\n",
    "        X (numpy array): Image data\n",
    "        y (numpy array): Labels\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load yawn images\n",
    "    for img_file in os.listdir(yawn_dir):\n",
    "        img_path = os.path.join(yawn_dir, img_file)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, img_size)\n",
    "            data.append(img)\n",
    "            labels.append(1)  # Label for yawn\n",
    "    \n",
    "    # Load no_yawn images\n",
    "    for img_file in os.listdir(no_yawn_dir):\n",
    "        img_path = os.path.join(no_yawn_dir, img_file)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, img_size)\n",
    "            data.append(img)\n",
    "            labels.append(0)  # Label for no_yawn\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(data)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def preprocess_data(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Preprocess data for training.\n",
    "    \n",
    "    Args:\n",
    "        X (numpy array): Image data\n",
    "        y (numpy array): Labels\n",
    "        test_size (float): Proportion of data to use for testing\n",
    "        random_state (int): Seed for random number generator\n",
    "        \n",
    "    Returns:\n",
    "        X_train (numpy array): Training images\n",
    "        X_test (numpy array): Testing images\n",
    "        y_train (numpy array): Training labels\n",
    "        y_test (numpy array): Testing labels\n",
    "    \"\"\"\n",
    "    # Reshape and normalize\n",
    "    X = X.reshape(-1, 64, 64, 1).astype('float32') / 255\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=random_state)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Set paths to your collected data\n",
    "yawn_dir = 'yawn'\n",
    "no_yawn_dir = 'no_yawn'\n",
    "\n",
    "# Load data\n",
    "X, y = load_data(yawn_dir, no_yawn_dir)\n",
    "\n",
    "# Preprocess data\n",
    "X_train, X_test, y_train, y_test = preprocess_data(X, y)\n",
    "\n",
    "# Save preprocessed data (optional but recommended)\n",
    "np.savez('preprocessed_data.npz', \n",
    "         X_train=X_train, \n",
    "         X_test=X_test, \n",
    "         y_train=y_train, \n",
    "         y_test=y_test)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49577d6c-d4d9-469c-9493-249f58b7cd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 10:27:23.987138: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-26 10:27:24.028030: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742965044.078038  213045 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742965044.093285  213045 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742965044.142515  213045 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742965044.142563  213045 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742965044.142572  213045 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742965044.142579  213045 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-26 10:27:24.158443: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/dhruv/AnacondaInstallation/envs/tf-env/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "I0000 00:00:1742965051.651386  213045 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2120 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742965058.088665  216311 service.cc:152] XLA service 0x71ccc8018590 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1742965058.088730  216311 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2025-03-26 10:27:38.198441: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1742965059.075260  216311 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5780 - loss: 0.6817 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742965065.351795  216311 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 985ms/step - accuracy: 0.5807 - loss: 0.6791 - val_accuracy: 0.6395 - val_loss: 0.6031\n",
      "Epoch 2/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6507 - loss: 0.5759 - val_accuracy: 0.8256 - val_loss: 0.3551\n",
      "Epoch 3/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8647 - loss: 0.3312 - val_accuracy: 0.9070 - val_loss: 0.2511\n",
      "Epoch 4/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9359 - loss: 0.1903 - val_accuracy: 0.9419 - val_loss: 0.1334\n",
      "Epoch 5/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9646 - loss: 0.1043 - val_accuracy: 0.9419 - val_loss: 0.1388\n",
      "Epoch 6/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9429 - loss: 0.1340 - val_accuracy: 0.9535 - val_loss: 0.0935\n",
      "Epoch 7/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9700 - loss: 0.0935 - val_accuracy: 0.9535 - val_loss: 0.0705\n",
      "Epoch 8/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9781 - loss: 0.0658 - val_accuracy: 0.9651 - val_loss: 0.0797\n",
      "Epoch 9/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9789 - loss: 0.0621 - val_accuracy: 0.9767 - val_loss: 0.0586\n",
      "Epoch 10/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9893 - loss: 0.0332 - val_accuracy: 0.9767 - val_loss: 0.0461\n",
      "Epoch 11/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9957 - loss: 0.0221 - val_accuracy: 0.9884 - val_loss: 0.0335\n",
      "Epoch 12/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9895 - loss: 0.0319 - val_accuracy: 0.9651 - val_loss: 0.0765\n",
      "Epoch 13/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9808 - loss: 0.0461 - val_accuracy: 0.9767 - val_loss: 0.0375\n",
      "Epoch 14/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9973 - loss: 0.0173 - val_accuracy: 1.0000 - val_loss: 0.0124\n",
      "Epoch 15/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9919 - loss: 0.0501 - val_accuracy: 0.9884 - val_loss: 0.0260\n",
      "Epoch 16/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9986 - loss: 0.0134 - val_accuracy: 0.9884 - val_loss: 0.0177\n",
      "Epoch 17/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9934 - loss: 0.0199 - val_accuracy: 0.9651 - val_loss: 0.0563\n",
      "3/3 - 0s - 46ms/step - accuracy: 1.0000 - loss: 0.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load preprocessed data\n",
    "data = np.load('preprocessed_data.npz')\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Set up early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[early_stop])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'\\nTest accuracy: {test_acc}')\n",
    "\n",
    "# Save the model\n",
    "model.save('yawn_detection_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a049490-9fac-4595-94a3-b101ea376463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp_zc5b8v_/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp_zc5b8v_/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmp_zc5b8v_'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 64, 64, 1), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  125126557915088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  125126557916320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  125126560244304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  125125622319936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  125126558061664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  125126558061488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  125126559691344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  125126559691168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  125126559777888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  125126559777712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1742965323.639924  213045 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1742965323.639963  213045 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-03-26 10:32:03.641066: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp_zc5b8v_\n",
      "2025-03-26 10:32:03.643324: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-03-26 10:32:03.643361: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmp_zc5b8v_\n",
      "I0000 00:00:1742965323.660166  213045 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n",
      "2025-03-26 10:32:03.663728: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-03-26 10:32:03.807063: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmp_zc5b8v_\n",
      "2025-03-26 10:32:03.843060: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 201999 microseconds.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained .h5 model\n",
    "model = tf.keras.models.load_model('yawn_detection_model.h5')\n",
    "\n",
    "# Convert to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted model\n",
    "with open('yawn_detection_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5b90f05-a3f5-402a-9de9-5c12d07575fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/AnacondaInstallation/envs/tf-env/lib/python3.9/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate FPS: 7.28\n",
      "Approximate FPS: 7.74\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path='yawn_detection_model.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Initialize webcam with optimized settings\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)  # Lower resolution for better performance\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize dlib's face detector and facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor_path = \"/home/dhruv/CollegeProject/Trial3/customYawndata/shape_predictor_68_face_landmarks.dat\"\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "def extract_mouth(image, landmarks):\n",
    "    mouth_points = []\n",
    "    for i in range(48, 68):\n",
    "        mouth_points.append((landmarks.part(i).x, landmarks.part(i).y))\n",
    "    mouth_points = np.array(mouth_points)\n",
    "    x, y, w, h = cv2.boundingRect(mouth_points)\n",
    "    mouth_roi = image[y:y+h, x:x+w]\n",
    "    return mouth_roi\n",
    "\n",
    "def predict_yawn(mouth_roi):\n",
    "    if mouth_roi.size == 0:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    mouth_roi = cv2.resize(mouth_roi, (64, 64))\n",
    "    mouth_roi = mouth_roi.reshape(1, 64, 64, 1) / 255.0\n",
    "    \n",
    "    # Set input tensor\n",
    "    interpreter.set_tensor(input_details[0]['index'], mouth_roi.astype(np.float32))\n",
    "    \n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Get output tensor\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    return \"Yawning\" if output[0][0] > 0.5 else \"Not Yawning\"\n",
    "\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        break\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces\n",
    "    faces = detector(gray)\n",
    "    \n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "        mouth_roi = extract_mouth(gray, landmarks)\n",
    "        status = predict_yawn(mouth_roi)\n",
    "        \n",
    "        x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, status, (x, y-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, \n",
    "                    (0, 255, 0) if status == \"Not Yawning\" else (0, 0, 255), 2)\n",
    "    \n",
    "    cv2.imshow('Yawn Detection', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Calculate and print FPS\n",
    "    frame_count += 1\n",
    "    if frame_count % 30 == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        fps = frame_count / elapsed_time\n",
    "        print(f\"Approximate FPS: {fps:.2f}\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792115bd-e5ad-4986-bed7-bf07b92b2e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
