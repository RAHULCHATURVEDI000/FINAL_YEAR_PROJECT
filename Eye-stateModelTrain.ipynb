{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d08d576f-1040-48a4-8072-baffb4e0aa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50937 images belonging to 2 classes.\n",
      "Found 16980 images belonging to 2 classes.\n",
      "Found 16981 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data generators with augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow from directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'data/train',\n",
    "    target_size=(36, 26),\n",
    "    batch_size=32,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    'data/val',\n",
    "    target_size=(36, 26),\n",
    "    batch_size=32,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'data/test',\n",
    "    target_size=(36, 26),\n",
    "    batch_size=32,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "455d738f-3043-445e-8d67-85bb5b87bf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/AnacondaInstallation/envs/tf-env/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(36, 26, 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    \n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    \n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed532b6f-2497-4584-89f7-abe9700f4676",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/AnacondaInstallation/envs/tf-env/lib/python3.9/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742952744.520530   98213 service.cc:152] XLA service 0x795ffc003460 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1742952744.520580   98213 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "I0000 00:00:1742952746.056477   98213 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   3/1592\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 68ms/step - accuracy: 0.5399 - loss: 0.9101  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742952755.232549   98213 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8878 - loss: 0.2781  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 106ms/step - accuracy: 0.8878 - loss: 0.2781 - val_accuracy: 0.9558 - val_loss: 0.1098\n",
      "Epoch 2/100\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 86ms/step - accuracy: 0.9591 - loss: 0.1098 - val_accuracy: 0.9323 - val_loss: 0.1930\n",
      "Epoch 3/100\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 85ms/step - accuracy: 0.9680 - loss: 0.0889 - val_accuracy: 0.9664 - val_loss: 0.1130\n",
      "Epoch 4/100\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9721 - loss: 0.0777  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 85ms/step - accuracy: 0.9721 - loss: 0.0777 - val_accuracy: 0.9764 - val_loss: 0.0630\n",
      "Epoch 5/100\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 86ms/step - accuracy: 0.9736 - loss: 0.0742 - val_accuracy: 0.9617 - val_loss: 0.0977\n",
      "Epoch 6/100\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9744 - loss: 0.0697  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 84ms/step - accuracy: 0.9744 - loss: 0.0697 - val_accuracy: 0.9853 - val_loss: 0.0426\n",
      "Epoch 7/100\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 85ms/step - accuracy: 0.9770 - loss: 0.0646 - val_accuracy: 0.9303 - val_loss: 0.1755\n",
      "Epoch 8/100\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 85ms/step - accuracy: 0.9736 - loss: 0.0737 - val_accuracy: 0.9803 - val_loss: 0.0517\n",
      "Epoch 9/100\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 85ms/step - accuracy: 0.9756 - loss: 0.0706 - val_accuracy: 0.9771 - val_loss: 0.0659\n",
      "Epoch 10/100\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 85ms/step - accuracy: 0.9765 - loss: 0.0650 - val_accuracy: 0.9815 - val_loss: 0.0510\n",
      "Epoch 11/100\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 84ms/step - accuracy: 0.9790 - loss: 0.0582 - val_accuracy: 0.9811 - val_loss: 0.0517\n",
      "\u001b[1m531/531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 46ms/step - accuracy: 0.9869 - loss: 0.0441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04193968325853348, 0.9851009845733643]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e3ea6ae-e05b-47d6-a61a-251a222c3923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 01:08:17.484578: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-29 01:08:17.523939: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743190697.571085  214495 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743190697.585623  214495 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743190697.616330  214495 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743190697.616368  214495 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743190697.616373  214495 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743190697.616376  214495 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-29 01:08:17.625371: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1743190703.663788  214495 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2120 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpy28y294v/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpy28y294v/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpy28y294v'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 36, 26, 1), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  138203238884624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203238883568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203238296384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203238306032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203238294800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203238296208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203237887488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203237887136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203237962144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203237962320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203237944304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203237944480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203238043488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203238035696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203238121136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203199303744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203238120608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203238120784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203199415040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203199414864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203199476128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203199476480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203199471456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203199475952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203199136864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203199136688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203199291984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  138203199291808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1743190707.263363  214495 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1743190707.263393  214495 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-03-29 01:08:27.264756: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpy28y294v\n",
      "2025-03-29 01:08:27.267773: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-03-29 01:08:27.267810: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpy28y294v\n",
      "I0000 00:00:1743190707.295161  214495 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n",
      "2025-03-29 01:08:27.300982: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-03-29 01:08:27.486206: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpy28y294v\n",
      "2025-03-29 01:08:27.535161: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 270411 microseconds.\n",
      "2025-03-29 01:08:27.583966: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "/home/dhruv/AnacondaInstallation/envs/tf-env/lib/python3.9/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/dhruv/AnacondaInstallation/envs/tf-env/lib/python3.9/site-packages/cv2/qt/plugins\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 4.64\n",
      "FPS: 10.03\n",
      "FPS: 10.03\n",
      "FPS: 9.94\n",
      "FPS: 9.97\n",
      "FPS: 10.03\n",
      "FPS: 10.00\n",
      "FPS: 9.96\n",
      "FPS: 10.06\n",
      "FPS: 9.94\n",
      "FPS: 10.09\n",
      "FPS: 9.98\n",
      "FPS: 9.99\n",
      "FPS: 10.13\n",
      "FPS: 9.87\n",
      "FPS: 10.12\n",
      "FPS: 9.98\n",
      "FPS: 9.97\n",
      "FPS: 10.11\n",
      "FPS: 9.90\n",
      "FPS: 9.97\n",
      "FPS: 10.03\n",
      "FPS: 10.01\n",
      "FPS: 9.98\n",
      "FPS: 10.00\n",
      "FPS: 10.09\n",
      "FPS: 10.07\n",
      "FPS: 9.90\n",
      "FPS: 10.08\n",
      "FPS: 9.98\n",
      "FPS: 9.95\n",
      "FPS: 9.97\n",
      "FPS: 10.19\n",
      "FPS: 9.93\n",
      "FPS: 10.02\n",
      "FPS: 10.02\n",
      "FPS: 10.00\n",
      "FPS: 9.93\n",
      "FPS: 10.01\n",
      "FPS: 10.07\n",
      "FPS: 9.99\n",
      "FPS: 9.98\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# Load optimized model\n",
    "model = load_model('/home/dhruv/Driver_Drowsiness_detection(CNN multithreading)/models/eye_model.h5')\n",
    "\n",
    "# Convert model to TensorFlow Lite for mobile/edge optimization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path='model.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Initialize face and eye detectors\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "# Stabilization parameters\n",
    "SMOOTHING_WINDOW = 7  # Number of frames to consider for smoothing\n",
    "OPEN_THRESHOLD = 0.6  # Confidence threshold for open eyes\n",
    "CLOSED_THRESHOLD = 0.4  # Confidence threshold for closed eyes\n",
    "PERSISTENCE_FRAMES = 3  # Number of frames to maintain state\n",
    "\n",
    "state_buffer = []\n",
    "current_state = \"Open\"\n",
    "state_counter = 0\n",
    "closed_counter = 0\n",
    "\n",
    "def preprocess_eye(eye_roi):\n",
    "    eye_roi = cv2.resize(eye_roi, (26, 36))\n",
    "    eye_roi = cv2.cvtColor(eye_roi, cv2.COLOR_BGR2GRAY)\n",
    "    eye_roi = cv2.GaussianBlur(eye_roi, (3, 3), 0)\n",
    "    eye_roi = eye_roi.reshape(1, 36, 26, 1).astype(np.float32) / 255.0\n",
    "    return eye_roi\n",
    "\n",
    "def detect_eye_status(frame):\n",
    "    global state_buffer, current_state, state_counter, closed_counter\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray, scaleFactor=1.1, minNeighbors=3, minSize=(15, 15))\n",
    "        \n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            eye_roi = frame[y+ey:y+ey+eh, x+ex:x+ex+ew]\n",
    "            \n",
    "            if eye_roi.size == 0:\n",
    "                continue\n",
    "                \n",
    "            processed_eye = preprocess_eye(eye_roi)\n",
    "            \n",
    "            # Use TFLite for faster inference\n",
    "            interpreter.set_tensor(input_details[0]['index'], processed_eye)\n",
    "            interpreter.invoke()\n",
    "            prediction = interpreter.get_tensor(output_details[0]['index'])[0][0]\n",
    "            \n",
    "            # Temporal smoothing\n",
    "            state_buffer.append(prediction)\n",
    "            if len(state_buffer) > SMOOTHING_WINDOW:\n",
    "                state_buffer.pop(0)\n",
    "            \n",
    "            avg_prediction = sum(state_buffer) / len(state_buffer)\n",
    "            \n",
    "            # Determine new state based on confidence\n",
    "            if avg_prediction < OPEN_THRESHOLD:\n",
    "                new_state = \"Open\"\n",
    "                closed_counter = 0\n",
    "            elif avg_prediction > CLOSED_THRESHOLD:\n",
    "                new_state = \"Closed\"\n",
    "                closed_counter += 1\n",
    "            else:\n",
    "                new_state = current_state\n",
    "            \n",
    "            # State persistence\n",
    "            if new_state != current_state:\n",
    "                state_counter += 1\n",
    "                if state_counter >= PERSISTENCE_FRAMES:\n",
    "                    current_state = new_state\n",
    "                    state_counter = 0\n",
    "            else:\n",
    "                state_counter = 0\n",
    "            \n",
    "            # Special handling for closed eyes\n",
    "            if closed_counter >= 2:\n",
    "                current_state = \"Closed\"\n",
    "                closed_counter = 0\n",
    "                state_counter = 0\n",
    "            \n",
    "            # Determine color based on state\n",
    "            color = (0, 255, 0) if current_state == \"Open\" else (0, 0, 255)\n",
    "            \n",
    "            cv2.rectangle(frame, (x+ex,y+ey), (x+ex+ew,y+ey+eh), color, 1)\n",
    "            cv2.putText(frame, current_state, (x+ex,y+ey-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "# Real-time webcam feed with performance improvements\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Downscale frame for faster processing\n",
    "    frame = cv2.resize(frame, (320, 240))\n",
    "    \n",
    "    output = detect_eye_status(frame)\n",
    "    \n",
    "    # Calculate FPS\n",
    "    frame_count += 1\n",
    "    if frame_count % 10 == 0:\n",
    "        end_time = time.time()\n",
    "        fps = frame_count / (end_time - start_time)\n",
    "        print(f\"FPS: {fps:.2f}\")\n",
    "        start_time = end_time\n",
    "        frame_count = 0\n",
    "    \n",
    "    cv2.imshow('Eye Status Detection', output)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea790f2-89dc-4880-9c6f-41d43863fcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
